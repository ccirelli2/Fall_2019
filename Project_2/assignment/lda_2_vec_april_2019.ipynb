{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccirelli2/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Run in terminal or command prompt to download spacy dict\n",
    "# python3 -m spacy download en\n",
    "# copyright Felipe Castrollio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "# topics library\n",
    "from topics import prepare_topics\n",
    "from topics import print_top_words_per_topic\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Keras tools\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import layers\n",
    "from keras.engine.input_layer import Input\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "\n",
    "window_size = 10\n",
    "epochs = 200000\n",
    "n_topics = 22 # for lda model\n",
    "vector_dim = 100\n",
    "batch_size = 1000\n",
    "\n",
    "# validation \n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 50  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/home/ccirelli2/Desktop/GSU/Fall_2019/Project_2/M_fund.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accession#</th>\n",
       "      <th>filing_year</th>\n",
       "      <th>principal_strategies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001193125-10-099241</td>\n",
       "      <td>2010</td>\n",
       "      <td>normally, the portfolio invests at least 80% o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001193125-10-099234</td>\n",
       "      <td>2010</td>\n",
       "      <td>normally, the portfolio invests at least 80% o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001193125-10-099244</td>\n",
       "      <td>2010</td>\n",
       "      <td>normally, the portfolio invests at least 80% o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001193125-10-099303</td>\n",
       "      <td>2010</td>\n",
       "      <td>normally, the portfolio invests at least 80% o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001193125-10-099357</td>\n",
       "      <td>2010</td>\n",
       "      <td>the portfolio employs a “passive management,” ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accession#  filing_year  \\\n",
       "0  0001193125-10-099241         2010   \n",
       "1  0001193125-10-099234         2010   \n",
       "2  0001193125-10-099244         2010   \n",
       "3  0001193125-10-099303         2010   \n",
       "4  0001193125-10-099357         2010   \n",
       "\n",
       "                                principal_strategies  \n",
       "0  normally, the portfolio invests at least 80% o...  \n",
       "1  normally, the portfolio invests at least 80% o...  \n",
       "2  normally, the portfolio invests at least 80% o...  \n",
       "3  normally, the portfolio invests at least 80% o...  \n",
       "4  the portfolio employs a “passive management,” ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14621"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        normally, the portfolio invests at least 80% o...\n",
      "1        normally, the portfolio invests at least 80% o...\n",
      "2        normally, the portfolio invests at least 80% o...\n",
      "3        normally, the portfolio invests at least 80% o...\n",
      "4        the portfolio employs a “passive management,” ...\n",
      "5        investing in the stock, bond and money market ...\n",
      "6        investing in the stock, bond and money market ...\n",
      "7        normally, the portfolio invests at least 80% o...\n",
      "8        normally, the portfolio invests at least 80% o...\n",
      "9        the portfolio invests only in high quality, sh...\n",
      "10       normally, the portfolio invests at least 80% o...\n",
      "11       normally, the portfolio invests at least 80% o...\n",
      "12       normally, the portfolio will invest at least 8...\n",
      "13       normally, the portfolio invests at least 80% o...\n",
      "14       normally, the portfolio invests at least 80% o...\n",
      "15       the portfolio employs a “passive management,” ...\n",
      "16       the portfolio invests primarily in the equity ...\n",
      "17       the portfolio invests primarily in the equity ...\n",
      "18                                                     NaN\n",
      "19                                                     NaN\n",
      "20                                                     NaN\n",
      "21                                                     NaN\n",
      "22                                                     NaN\n",
      "23                                                     NaN\n",
      "24                                                     NaN\n",
      "25                                                     NaN\n",
      "26                                                     NaN\n",
      "27                                                     NaN\n",
      "28                                                     NaN\n",
      "29                                                     NaN\n",
      "                               ...                        \n",
      "14591    the adviser seeks to achieve the fund’s invest...\n",
      "14592    the fund seeks to achieve its objective by inv...\n",
      "14593    the fund seeks to achieve its objective by inv...\n",
      "14594    the fund seeks to achieve its objective by inv...\n",
      "14595    strategy the portfolio seeks to achieve its in...\n",
      "14596    the fund seeks capital appreciation over a ful...\n",
      "14597    the fund seeks to achieve its investment objec...\n",
      "14598                                                  NaN\n",
      "14599    of the fund in an effort to provide equity-lik...\n",
      "14600    the fund’s investment style is flexible and in...\n",
      "14601    under normal market conditions, the portfolio ...\n",
      "14602    the fund is actively managed by the fund’s inv...\n",
      "14603    the fund will attempt to achieve its investmen...\n",
      "14604    the fund seeks to achieve its investment objec...\n",
      "14605    investment strategies the fund pursues its obj...\n",
      "14606    in seeking long-term capital appreciation over...\n",
      "14607    under normal market conditions, the fund inves...\n",
      "14608    the fund may invest in corporate bonds, u.s. t...\n",
      "14609    ivy wilshire global allocation fund is a fund-...\n",
      "14610    the fund seeks to achieve its investment goal ...\n",
      "14611    the fund seeks to achieve its investment objec...\n",
      "14612    columbia management investment advisers, llc (...\n",
      "14613    columbia management investment advisers, llc (...\n",
      "14614    columbia management investment advisers, llc (...\n",
      "14615    columbia management investment advisers, llc (...\n",
      "14616    columbia management investment advisers, llc (...\n",
      "14617    the fund invests substantially all of its asse...\n",
      "14618    under normal circumstances, at least 80% of th...\n",
      "14619    of the fund the fund seeks long-term capital a...\n",
      "14620    the fund aims to achieve capital growth by inv...\n",
      "Name: principal_strategies, Length: 14621, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['principal_strategies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ for explanation on data processing steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<input>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-19-68c72f5339e7>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "  data = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data]\n",
      "<ipython-input-19-68c72f5339e7>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "  data = [re.sub('\\s+', ' ', str(sent)) for sent in data]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'principal_risks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-68c72f5339e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprincipal_risks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Remove Emails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\S*@\\S*\\s?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'principal_risks'"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.principal_risks.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', str(sent)) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', str(sent)) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", str(sent)) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "#print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load(\"c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\en_core_web_sm\\\\en_core_web_sm-2.0.0\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(len(data_lemmatized))\n",
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize every doc\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_lemmatized)\n",
    "sequences = tokenizer.texts_to_sequences(data_lemmatized)\n",
    "n_documents = len(sequences)\n",
    "\n",
    "dictionary = tokenizer.word_index\n",
    "dictionary[\"null\"] = 0\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "vocab_size = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset: word pairs and doc ids with positive and negative samples\n",
    "\n",
    "window_size = 2\n",
    "targets = []\n",
    "contexts = []\n",
    "labels = []\n",
    "couples = []\n",
    "doc_ids = []\n",
    "\n",
    "for i in range(0,n_documents):\n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        print (i)\n",
    "    seq = sequences[i]\n",
    "    sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "    couple, label = skipgrams(seq, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "    if not couple:\n",
    "        next\n",
    "    try:\n",
    "        target, context = zip(*couple)\n",
    "        targets = targets + list(target)\n",
    "        contexts = contexts + list(context)\n",
    "        doc_ids = doc_ids + [i]*len(context)\n",
    "        labels = labels + label\n",
    "        couples = couples + couple\n",
    "    except:\n",
    "        print (\"Error on \" + str(seq))\n",
    "    \n",
    "data_target = np.array(targets, dtype='int32')\n",
    "data_context = np.array(contexts, dtype='int32')\n",
    "doc_ids = np.array(doc_ids, dtype='int32')\n",
    "labels = np.array(labels, dtype='int32')\n",
    "\n",
    "# split into train and test\n",
    "\n",
    "from random import sample\n",
    "training_split = 0.8\n",
    "l = len(data_target) #length of data \n",
    "f = int(l * training_split) #number of elements you need\n",
    "indices = sample(range(l),f)\n",
    "\n",
    "train_data_target = data_target[indices]\n",
    "test_data_target = np.delete(data_target,indices)\n",
    "train_data_context = data_context[indices]\n",
    "test_data_context = np.delete(data_context,indices)\n",
    "train_doc_ids = doc_ids[indices]\n",
    "test_doc_ids = np.delete(doc_ids,indices)\n",
    "train_labels = labels[indices]\n",
    "test_labels = np.delete(labels,indices)\n",
    "\n",
    "print(couples[:10], labels[:10], doc_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size of training data \" + str(len(train_data_target)))\n",
    "print(\"size of testing data \" + str(len(test_data_target)))\n",
    "print(\"size of labels \" + str(len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is where we start creating the model. The model consists of two parallel flows: word embedding (like word2vec) and topic embedding (like LDA). Please refer to the model image here: https://github.com/cemoody/lda2vec. You can see on the left the word embedding happens, and on the right the topic lda embedding happens. At the bottom the two vectors are added together to form the final context_vector. \n",
    "\n",
    "The model will have three training inputs: \n",
    "    1) input_context: pivot word\n",
    "    2) input_target: word that we are trying to predict\n",
    "    3) input_doc: document id \n",
    "\n",
    "And one training output:\n",
    "    1) label: 0 or 1 which defines if input_context and input_target are similar taking into account input_doc\n",
    "    \n",
    "The model predictions are gien by \"preds\" which will output a similarity score between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input placeholder variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "input_doc = Input((1,), dtype='int32')\n",
    "labels = Input((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word2`vec layers\n",
    "embedding = layers.Embedding(vocab_size, vector_dim, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "word_context = embedding(input_context)\n",
    "word_context = Reshape((vector_dim, 1))(word_context)\n",
    "word_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lda layers\n",
    "\n",
    "scalar = 1 / np.sqrt(n_documents + n_topics)\n",
    "all_doc_topics_embedding =(tf.Variable(tf.random_normal([n_documents, n_topics], mean=0, stddev=50*scalar),name=\"doc_embeddings\",trainable=True))  # Gaussian distribution\n",
    "#all_doc_topics_embedding = keras.layers.Embedding(n_documents, n_topics, embeddings_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=1 / np.sqrt(n_documents + n_topics), seed=None))\n",
    "#doc_topics = all_doc_topics_embedding(input_doc)\n",
    "def embedding_lookup(x):\n",
    "    ind = tf.cast(x, tf.int32)\n",
    "    return tf.nn.embedding_lookup(all_doc_topics_embedding,ind,partition_strategy='mod',name=\"doc_proportions\")\n",
    "\n",
    "#doc_topics = all_doc_topics_embedding(input_doc)\n",
    "print(input_doc)\n",
    "doc_topics = keras.layers.Lambda(embedding_lookup)(input_doc)\n",
    "doc_topics_norm = keras.layers.Activation(activation=\"softmax\")(doc_topics)\n",
    "#all_doc_topics_norm = keras.layers.Activation(activation=\"softmax\",name=\"all_doc_topics_norm\")(all_doc_topics)\n",
    "#doc_topics_norm = keras.layers.Lambda(embedding_lookup)(input_doc)\n",
    "transform = keras.layers.Dense(vector_dim, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "topic_context = transform(doc_topics_norm)\n",
    "topic_context = Reshape((vector_dim, 1))(topic_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine context layers\n",
    "context = keras.layers.Add()([word_context, topic_context])\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure between target and context\n",
    "similarity = layers.dot([target, context], axes=1, normalize=True)\n",
    "similarity = Reshape((1,))(similarity)\n",
    "\n",
    "# add the sigmoid output layer\n",
    "preds = Dense(1, activation='sigmoid', name='similarity')(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defnie custom loss functions\n",
    "\n",
    "# lda loss model\n",
    "lmbda = 1.0\n",
    "fraction = 1/100000\n",
    "alpha = None # defaults to 1/n_topics\n",
    "\n",
    "\n",
    "def dirichlet_likelihood(weights, alpha=None):\n",
    "    \n",
    "    num_topics = n_topics\n",
    "    \n",
    "    if alpha is None:\n",
    "        alpha = 1 / num_topics\n",
    "\n",
    "    log_proportions = tf.nn.log_softmax(weights)\n",
    "\n",
    "    loss = (alpha - 1) * log_proportions\n",
    "\n",
    "    #return -tf.reduce_sum(loss) # log-sum-exp\n",
    "    return tf.reduce_sum(loss) # log-sum-exp\n",
    "\n",
    "def loss_lda(y_pred, y_true, topics_layer):\n",
    "    return lmbda*fraction*dirichlet_likelihood(topics_layer)\n",
    "\n",
    "def loss_word2vec(y_pred, y_true):\n",
    "    #return tf.math.add(tf.math.multiply(y_true, (-tf.math.log(y_pred))), \n",
    "    #                   tf.math.multiply((1 - y_true),(-tf.math.log(1 - y_pred))))\n",
    "    return keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "# lda2vec loss\n",
    "def loss_sum(y_pred, y_true, topics_layer):\n",
    "    word2vec_loss = loss_word2vec(y_pred, y_true)\n",
    "    lda_loss = loss_lda(y_pred, y_true, topics_layer)\n",
    "    sum_loss = word2vec_loss + lda_loss\n",
    "    return sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluation models which are used to print out similar words during training.\n",
    "# This is not needed for model training, but is used to check model outputs periodically to see if model is working\n",
    "\n",
    "topic_context = Input(shape=(vector_dim, ))\n",
    "topic_similarity = layers.dot([topic_context, word_context], axes=0)\n",
    "topic2words_model = Model(input=[topic_context,input_context], output=topic_similarity)\n",
    "\n",
    "words_similarity = layers.dot([target, word_context], axes=1, normalize=True)\n",
    "nearby_words_model = Model(input=[input_target, input_context], output=words_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_words =['144a','active','adviser','allocation','country','region','arbitrage', \\\n",
    "      'asset','banking','index','passive','beta','bond', 'debt','brokerage', \\\n",
    "      'call','capitalization','cash','commodity','equity','close','collateral',\\\n",
    "      'company', 'interest','sector','conflict','investment', 'obligations', \\\n",
    "      'diversification','market','counterparty','currency','cybersecurity', \\\n",
    "      'derivative', 'diversification', 'expense','volatility','future','government', \\\n",
    "      'hedge', 'liquidity','insurance','issuer','legal','leverage','target', 'date',  \\\n",
    "      'fund','management','mortgage','over','counter','turnover','estate','settlement', \\\n",
    "      'short','swap','tax','yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions to print similar words given a topic and similar words given another word\n",
    "# This is not used for training, but for periodic evaluation of the model\n",
    "\n",
    "class TopicSimilarityCallback:\n",
    "    def run_sim(self, topics):\n",
    "        for i in range(n_topics):\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(topics[i])\n",
    "            nearest = (-sim).argsort()[0:top_k + 1]\n",
    "            log_str = 'Closest words to topic %d:' % i\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_sim(topic):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.reshape(topic,(1,-1))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = topic2words_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "t_sim_cb = TopicSimilarityCallback()\n",
    "\n",
    "\n",
    "class WordsSimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 10  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[0:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = nearby_words_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "w_sim_cb = WordsSimilarityCallback()\n",
    "\n",
    "class SpecificWordsSimilarityCallback:\n",
    "    def run_sim(self, word):\n",
    "        if word not in dictionary:\n",
    "            print('Nearest to %s: Word does not exist in dictionary' % word)\n",
    "            return\n",
    "        word_index = dictionary[word]\n",
    "        top_k = 10  # number of nearest neighbors\n",
    "        sim = self._get_sim(word_index)\n",
    "        nearest = (-sim).argsort()[0:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % word\n",
    "        for k in range(top_k):\n",
    "            close_word = reverse_dictionary[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = nearby_words_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sw_sim_cb = SpecificWordsSimilarityCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS PART IS NOT BEING USED\n",
    "# train model using keras (not being used since lda is not training)\n",
    "\n",
    "def train_with_keras(): \n",
    "    \n",
    "    model = Model(inputs=[input_target, input_context, input_doc], outputs=preds)\n",
    "    model.compile(loss=loss_lda2vec(all_doc_topics_embedding), metrics=[loss_word2vec, loss_lda], optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    \n",
    "    iterations = 100000\n",
    "    batch_size = 100\n",
    "    train_loss_every = 100\n",
    "    test_loss_every = 10000\n",
    "    sum_loss = 0\n",
    "    word2vec_loss = 0\n",
    "    lda_loss = 0\n",
    "\n",
    "    for cnt in range(iterations):\n",
    "\n",
    "        # print out training loss\n",
    "        if cnt % train_loss_every == 0 and cnt > 0:\n",
    "            print(\"Iteration {}, average sum_loss={}, average word2vec_loss={}, average lda_loss={}\".format(cnt, sum_loss/train_loss_every, word2vec_loss/train_loss_every, lda_loss/train_loss_every))\n",
    "            print(all_doc_topics_embedding.get_weights()[0][1])\n",
    "            print(K.eval(dirichlet_likelihood(all_doc_topics_embedding))*fraction)\n",
    "            print(softmax(all_doc_topics_embedding.get_weights()[0][0]))\n",
    "\n",
    "        # print out test loss and similar words\n",
    "        if cnt % test_loss_every == 0 and cnt > 0:\n",
    "            t_sim_cb.run_sim(transform.get_weights()[0])\n",
    "            w_sim_cb.run_sim()\n",
    "            test_loss = model.evaluate(x=[test_data_target, test_data_context, test_doc_ids], y=test_labels)\n",
    "            print(\"Iteration {}, test_loss={}\\n\".format(cnt, test_loss))\n",
    "\n",
    "        # training happens here\n",
    "        idx = np.random.randint(0, len(train_labels)-1, batch_size).tolist()\n",
    "        loss =  model.fit(x=[train_data_target[idx], train_data_context[idx], train_doc_ids[idx]], y=train_labels[idx],epochs=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I couldn't get the model to train in Keras (error in LDA loss), so I trained it with TensorFlow instead. \n",
    "# Remember that Keras is running TensorFlow in the back end. This is a bit messy since the model was created \n",
    "#in Keras language, but it works well.\n",
    "\n",
    "import math\n",
    "batch_size = 150\n",
    "train_loss_every = 500\n",
    "test_loss_every = 10000\n",
    "\n",
    "# define loss functions to compute\n",
    "loss = loss_sum(preds, labels, all_doc_topics_embedding)\n",
    "loss_topics = loss_lda(preds, labels, all_doc_topics_embedding)\n",
    "loss_words = loss_word2vec(preds, labels)\n",
    "\n",
    "# define gradient descent and initialize variables\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = K.get_session() # get session from keras\n",
    "sess.run(init_op)\n",
    "\n",
    "# Run training loop\n",
    "with sess.as_default():\n",
    "    for i in range(200000):\n",
    "        \n",
    "        idx = np.random.randint(0, len(train_labels)-1, batch_size).tolist()\n",
    "        # training happens here\n",
    "        losses = sess.run([train_step, loss,loss_topics, loss_words,preds,labels], feed_dict= {input_target:np.reshape(train_data_target[idx],(-1,1)),\n",
    "                                   input_context:np.reshape(train_data_context[idx],(-1,1)),\n",
    "                                   input_doc:np.reshape(train_doc_ids[idx],(-1,1)),\n",
    "                                   labels: np.reshape(train_labels[idx],(-1,1))})\n",
    "            \n",
    "        # print training loss\n",
    "        if i % train_loss_every == 0:\n",
    "            print(\"Iteration {}, average sum_loss={}, average lda_loss={}, average w2v_loss={}\".format(i,np.mean(losses[1]),np.mean(losses[2]),np.mean(losses[3])))\n",
    "        \n",
    "        # print test\\loss and similar words\n",
    "        if i % test_loss_every == 0:\n",
    "            test_loss = sess.run([loss], feed_dict= {input_target:np.reshape(test_data_target,(-1,1)),\n",
    "                                   input_context:np.reshape(test_data_context,(-1,1)),\n",
    "                                   input_doc:np.reshape(test_doc_ids,(-1,1)),\n",
    "                                   labels: np.reshape(test_labels,(-1,1))})\n",
    "            print(\"\\n\\n******Iteration {}, test_loss={}****\\n\\n\".format(i, np.mean(test_loss[0])))\n",
    "            #w_sim_cb.run_sim()\n",
    "            t_sim_cb.run_sim(transform.get_weights()[0])\n",
    "            for word in find_similar_words:\n",
    "                sw_sim_cb.run_sim(word)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary['market']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
